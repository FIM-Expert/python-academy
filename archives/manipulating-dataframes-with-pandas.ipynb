{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:maroon\">**Manipulating DataFrames with Pandas**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## <span style=\"color:blue\">**Functions and Methods**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### DataFrame slicing\n",
    "`df[\"column_label\"][\"row_label\"]`  \n",
    "`df.loc[\"row_label\", \"column_label\"]`  \n",
    "`df.iloc[row_index, column_index]`\n",
    "  \n",
    "`df[\"column_label\"]` **--> pandas Series**  \n",
    "`df[[\"column_label\"]` **--> pandas DataFrame**  \n",
    "`df[[\"column_label_1\", \"column_label_2\", et...]]` **--> pandas DataFrame with only the column labels provided**  \n",
    "\n",
    "`df[\"row_start\":\"row_end\"]` **--> pandas DataFrame for rows between start and end inclusive**  \n",
    "`df[\"row_end\":\"row_start\":-1]` **--> pandas DataFrame for rows between start and end in reverse order inclusive**  \n",
    "`df.loc[:, :\"column_rightmost\"]` **--> pandas DataFrame for all rows all columns from the left to rightmost inclusive**  \n",
    "`df.loc[:, \"column_start:\"column_end\"]` **--> pandas DataFrame for all rows all columns between start and end inclusive**  \n",
    "`df.loc[row_list, column_list]` **--> pandas DataFrame for labels in row_list and column_list (this is a highly configured slice)**  \n",
    "\n",
    "#### When a boolean series is used to slice a dataframe, it is called a filter\n",
    "`boolean_series = df[\"column_label\"] > value`  \n",
    "`df_filtered = df[boolean_series]`  \n",
    "`df[\"column_label_1\"][boolean_series] = assign_new_value` **--> assign a new value to a column using a row-based boolean filter**  \n",
    "\n",
    "#### Dropping data\n",
    "`df.dropna(how=\"any\")` **--> drops rows in df where any column values is NaN**  \n",
    "`df.dropna(how=\"all\")` **--> drops rows in df where all column values are NaN**  \n",
    "`df.dropna(thres=threshold_value, axis=\"columns\")` **--> drop columns with less than threshold_value non-missing values**  \n",
    "\n",
    "#### Tranforming data\n",
    "`df.apply(func)` **--> applies func to every element in df**  \n",
    "\n",
    "##### <span style=\"color:red\">**Example of using .map()**</span>\n",
    "`red_vs_blue = {\"Obama\": \"blue\", \"Romney\": \"red\"}` **--> dictionary with keys corresponding to the categorical values that you want to map**  \n",
    "`election[\"color\"] = election[\"winner\"].map(red_vs_blue)` **--> # use the dictionary to map the \"winner\" column to the new column \"color\"  \n",
    "\n",
    "##### <span style=\"color:red\">**Vectorizing over looping**</span>\n",
    "`df.floordiv(12)` **--> this is a pandas method that utilizes vectorization**  \n",
    "\n",
    "`numpy.floordiv(df, 12)` **--> this is a numpy ufunc (universal function) that also utilizes vectorization**  \n",
    "\n",
    ">`def dozens(n):  \n",
    "      return n // 12`\n",
    ">\n",
    ">`df.apply(dozens)`  \n",
    "\n",
    "`df.apply(lambda n: n // 12)`\n",
    "\n",
    "##### Vectorized methods work on pandas Series as well\n",
    "`df.index = df.index.str.upper()`  \n",
    "\n",
    "`df.index = df.index.map(str.upper)`  **--> for a DataFrame index, .map applies a function to the elements in an index**  \n",
    "\n",
    "##### Example of using a function in a vectorized manner\n",
    ">`from scipy.stats import zscore  \n",
    " turnout_zscore = zscore(election[\"turnout\"])  \n",
    " election[\"turnout_zscore\"] = turnout_zscore`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## <span style=\"color:blue\">**Advanced Indexing**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Key building blocks of pandas data structures\n",
    "1. indexes: sequences of labels, immutable (if you want to modify the index then you hneed to change the whole index)\n",
    "2. Series: 1D array with an associated index\n",
    "3. DataFrames: 2D array with Series as columns\n",
    "\n",
    "##### <span style=\"color:red\">**You should try to create data structures where indexes are unique (although this is not a requirement)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### <span style=\"color:red\">**Modifying an entire index**</span>\n",
    ">`new_idx = [x.upper() for x in df.index]`  \n",
    "`df.index = new_idx`  \n",
    "`df.index.name = \"index_name_label\"`  \n",
    "`df.columns.name = \"columns_name_label\"`  \n",
    "\n",
    "##### <span style=\"color:red\">**Creating a index from scratch**</span>\n",
    "`index_list` **--> list that you want to use to generate an index**  \n",
    "`df.index = index_list`  \n",
    "\n",
    "##### <span style=\"color:red\">**Hierarchical index (multi-index)**</span>\n",
    "`df.loc[[\"outer_index_label\", \"inner_index_label\"]]` **--> retrieves the relevant rows of df**  \n",
    "`df[\"outer_index_row_label_start\":\"outer_index_row_label_end\"]` **--> retrieves relevant rows between start and end inclusive**  \n",
    "`df = df.set_index([\"outer_index_label\", \"inner_index_label\"])` **--> sets the multi-index for df**  \n",
    "`df = df.sort_index()`\n",
    "\n",
    "##### Accessing the outermost index works like single index slicing\n",
    "##### Accessing inner indices requires --> slice <-- This is going to require some practice!!!\n",
    "`df.loc[(\"outer_index_label\", \"inner_index_label\")]` **--> observe that a tuple is being passed to the indexer**  \n",
    "`df.loc[([\"outer_index_labels\"], \"inner_index_label\"), :]` **--> the tuple is defining the rows within the multi-index**  \n",
    "`df.loc[(slice(None), 2), :]` **--> slice(None) is removing filtering on the outer index**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Rearranging and Reshaping Data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**Pivot**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4])\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"A pivoted dataframe: df --> pivoted_users \\n\")\n",
    "pivoted_users = pd.pivot(data=users, index=\"weekday\", columns=\"city\", values=\"visitors\")\n",
    "print(pivoted_users, \"\\n\")\n",
    "\n",
    "print(\"A stratified pivoted dataframe: df --> stratified_users \\n\")\n",
    "stratified_users = pd.pivot(data=users, index=\"weekday\", columns=\"city\")\n",
    "print(stratified_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**Stack and Unstack**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4], index_col=[\"city\", \"weekday\"]).sort_index()\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"users \"\"weekday\"\" column is now unstacked: df --> unstacked_users \\n\")\n",
    "unstacked_users = users.unstack(\"weekday\")\n",
    "print(unstacked_users, \"\\n\")\n",
    "\n",
    "print(\"unstacked_users \"\"weekday\"\" column is now stacked again: df --> restacked_users \\n\")\n",
    "restacked_users = unstacked_users.stack(\"weekday\")\n",
    "print(restacked_users, \"\\n\")\n",
    "\n",
    "print(\"Are users and restacked_users identical?  \", users.equals(restacked_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4], index_col=[\"city\", \"weekday\"]).sort_index()\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"users \"\"city\"\" column is now unstacked: df --> unstacked_users \\n\")\n",
    "unstacked_users = users.unstack(\"city\")\n",
    "print(unstacked_users, \"\\n\")\n",
    "\n",
    "print(\"unstacked_users \"\"city\"\" column is now stacked again: df --> restacked_users \\n\")\n",
    "restacked_users = unstacked_users.stack(\"city\")\n",
    "print(restacked_users, \"\\n\")\n",
    "\n",
    "print(\"Are users and restacked_users identical?  \", users.equals(restacked_users), \"\\n\")\n",
    "\n",
    "print(\"swap levels 0 and 1: df --> swapped_levels_users \\n\")\n",
    "swapped_levels_users = restacked_users.swaplevel(0, 1)\n",
    "print(swapped_levels_users, \"\\n\")\n",
    "\n",
    "print(\"Are users and swapped_levels_users identical?  \", users.equals(swapped_levels_users), \"\\n\")\n",
    "\n",
    "print(\"swapped_levels_users index sorted: df --> swapped_levels_users \\n\")\n",
    "swapped_levels_users = swapped_levels_users.sort_index()\n",
    "print(swapped_levels_users, \"\\n\")\n",
    "\n",
    "print(\"Are users and swapped_levels_users identical?  \", users.equals(swapped_levels_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**Melt**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3]).sort_values([\"city\", \"weekday\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"A pivoted dataframe: df --> pivoted_users \\n\")\n",
    "pivoted_users = users.pivot(index=\"weekday\", columns=\"city\", values=\"visitors\")\n",
    "print(pivoted_users, \"\\n\")\n",
    "\n",
    "print(\"A reset index dataframe: df --> reset_index_users \\n\")\n",
    "reset_index_users = pivoted_users.reset_index()\n",
    "print(reset_index_users, \"\\n\")\n",
    "\n",
    "print(\"A melted dataframe: df --> melted_users \\n\")\n",
    "melted_users = pd.melt(reset_index_users, id_vars=[\"weekday\"], value_name=\"visitors\")\n",
    "print(melted_users, \"\\n\")\n",
    "\n",
    "print(\"Are users and melted_users identical?  \", users.equals(melted_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4]).sort_values(by=[\"weekday\", \"city\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"A melted dataframe: df --> melted_users \\n\")\n",
    "melted_users = pd.melt(users, id_vars=[\"weekday\", \"city\"])\n",
    "print(melted_users, \"\\n\")\n",
    "\n",
    "print(\"A melted dataframe with nicer column names: df --> melted_users_2 \\n\")\n",
    "melted_users_2 = pd.melt(users, id_vars=[\"weekday\", \"city\"], var_name=\"metric\", value_name=\"count\")\n",
    "print(melted_users_2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4]).sort_values(by=[\"weekday\", \"city\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"An indexed dataframe: df --> indexed_users \\n\")\n",
    "indexed_users = users.set_index([\"city\", \"weekday\"])\n",
    "print(indexed_users, \"\\n\")\n",
    "\n",
    "#print(\"A dataframe with data values shown in key-value pairs: df --> kv_users \\n\")\n",
    "kv_users = pd.melt(indexed_users, col_level=0, var_name=\"metric\", value_name=\"count\")\n",
    "print(kv_users, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**Pivot Table**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4]).sort_values(by=[\"weekday\", \"city\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"A stratified pivoted dataframe: df --> stratified_users \\n\")\n",
    "stratified_users = users.pivot_table(index=\"weekday\", columns=\"city\")\n",
    "print(stratified_users, \"\\n\")\n",
    "\n",
    "print(\"A use-case for aggfunc: df --> users_counted_1 \\n\")\n",
    "users_counted_1 = users.pivot_table(index=\"weekday\", aggfunc=\"count\")\n",
    "print(users_counted_1, \"\\n\")\n",
    "\n",
    "print(\"An identical implementation as above with use-case for aggfunc: df --> users_counted_2 \\n\")\n",
    "users_counted_2 = users.pivot_table(index=\"weekday\", aggfunc=len)\n",
    "print(users_counted_2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\"./data/users.csv\", usecols=[1,2,3,4]).sort_values(by=[\"weekday\", \"city\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"The starting dataframe: df --> users \\n\")\n",
    "print(users, \"\\n\")\n",
    "\n",
    "print(\"Generating sums by using aggrunc: df --> sum_by_weekday_users \\n\")\n",
    "sum_by_weekday_users = users.pivot_table(index=\"weekday\", aggfunc=sum)\n",
    "print(sum_by_weekday_users, \"\\n\")\n",
    "\n",
    "print(\"Generating sums with grand-totals using aggfunc: df --> sum_with_grand_totals_users \\n\")\n",
    "sum_with_grand_totals_users = users.pivot_table(index=\"weekday\", aggfunc=sum, margins=True)\n",
    "print(sum_with_grand_totals_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Grouping Data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**Group By**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting dataframe: df --> titanic \n",
      "\n",
      "      pclass  survived                                             name  \\\n",
      "0          1         1                    Allen, Miss. Elisabeth Walton   \n",
      "1          1         1                   Allison, Master. Hudson Trevor   \n",
      "2          1         0                     Allison, Miss. Helen Loraine   \n",
      "3          1         0             Allison, Mr. Hudson Joshua Creighton   \n",
      "4          1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)   \n",
      "...      ...       ...                                              ...   \n",
      "1304       3         0                             Zabour, Miss. Hileni   \n",
      "1305       3         0                            Zabour, Miss. Thamine   \n",
      "1306       3         0                        Zakarian, Mr. Mapriededer   \n",
      "1307       3         0                              Zakarian, Mr. Ortin   \n",
      "1308       3         0                               Zimmerman, Mr. Leo   \n",
      "\n",
      "         sex    age  sibsp  parch  ticket      fare    cabin embarked boat  \\\n",
      "0     female  29.00      0      0   24160  211.3375       B5        S    2   \n",
      "1       male   0.92      1      2  113781  151.5500  C22 C26        S   11   \n",
      "2     female   2.00      1      2  113781  151.5500  C22 C26        S  NaN   \n",
      "3       male  30.00      1      2  113781  151.5500  C22 C26        S  NaN   \n",
      "4     female  25.00      1      2  113781  151.5500  C22 C26        S  NaN   \n",
      "...      ...    ...    ...    ...     ...       ...      ...      ...  ...   \n",
      "1304  female  14.50      1      0    2665   14.4542      NaN        C  NaN   \n",
      "1305  female    NaN      1      0    2665   14.4542      NaN        C  NaN   \n",
      "1306    male  26.50      0      0    2656    7.2250      NaN        C  NaN   \n",
      "1307    male  27.00      0      0    2670    7.2250      NaN        C  NaN   \n",
      "1308    male  29.00      0      0  315082    7.8750      NaN        S  NaN   \n",
      "\n",
      "       body                        home.dest  \n",
      "0       NaN                     St Louis, MO  \n",
      "1       NaN  Montreal, PQ / Chesterville, ON  \n",
      "2       NaN  Montreal, PQ / Chesterville, ON  \n",
      "3     135.0  Montreal, PQ / Chesterville, ON  \n",
      "4       NaN  Montreal, PQ / Chesterville, ON  \n",
      "...     ...                              ...  \n",
      "1304  328.0                              NaN  \n",
      "1305    NaN                              NaN  \n",
      "1306  304.0                              NaN  \n",
      "1307    NaN                              NaN  \n",
      "1308    NaN                              NaN  \n",
      "\n",
      "[1309 rows x 14 columns] \n",
      "\n",
      "Group by pclass (note that this is a DataFrameGroupBy object): ob --> by_class \n",
      "\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000015018F6AE08> \n",
      "\n",
      "Calculate count by group pclass: sr --> count_by_pclass \n",
      "\n",
      "pclass\n",
      "1    323\n",
      "2    277\n",
      "3    709\n",
      "Name: survived, dtype: int64 \n",
      "\n",
      "Group by embarked and pclass (note that this is also a DataFrameGroupBy object): ob --> by_embarked_pclass \n",
      "\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000150194491C8> \n",
      "\n",
      "Calculate count by multi-group embarked and pclass: sr --> count_by_embarked_pclass \n",
      "\n",
      "embarked  pclass\n",
      "C         1         141\n",
      "          2          28\n",
      "          3         101\n",
      "Q         1           3\n",
      "          2           7\n",
      "          3         113\n",
      "S         1         177\n",
      "          2         242\n",
      "          3         495\n",
      "Name: survived, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"./data/titanic.csv\")\n",
    "\n",
    "print(\"The starting dataframe: df --> titanic \\n\")\n",
    "print(titanic, \"\\n\")\n",
    "\n",
    "print(\"Group by pclass (note that this is a DataFrameGroupBy object): ob --> by_class \\n\")\n",
    "by_class = titanic.groupby(\"pclass\")\n",
    "print(by_class, \"\\n\")\n",
    "\n",
    "print(\"Calculate count by group pclass: sr --> count_by_pclass \\n\")\n",
    "count_by_pclass = by_class[\"survived\"].count()\n",
    "print(count_by_pclass, \"\\n\")\n",
    "\n",
    "print(\"Group by embarked and pclass (note that this is also a DataFrameGroupBy object): ob --> by_embarked_pclass \\n\")\n",
    "by_embarked_pclass = titanic.groupby([\"embarked\", \"pclass\"])\n",
    "print(by_embarked_pclass, \"\\n\")\n",
    "\n",
    "print(\"Calculate count by multi-group embarked and pclass: sr --> count_by_embarked_pclass \\n\")\n",
    "count_by_embarked_pclass = by_embarked_pclass[\"survived\"].count()\n",
    "print(count_by_embarked_pclass, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting dataframe: df --> life \n",
      "\n",
      "           Life expectancy   1800   1801   1802   1803   1804   1805   1806  \\\n",
      "0                 Abkhazia    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "1              Afghanistan  28.21  28.20  28.19  28.18  28.17  28.16  28.15   \n",
      "2    Akrotiri and Dhekelia    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "3                  Albania  35.40  35.40  35.40  35.40  35.40  35.40  35.40   \n",
      "4                  Algeria  28.82  28.82  28.82  28.82  28.82  28.82  28.82   \n",
      "..                     ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "255             Yugoslavia    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "256                 Zambia  32.60  32.60  32.60  32.60  32.60  32.60  32.60   \n",
      "257               Zimbabwe  33.70  33.70  33.70  33.70  33.70  33.70  33.70   \n",
      "258                  Åland    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "259            South Sudan  26.67  26.67  26.67  26.67  26.67  26.67  26.67   \n",
      "\n",
      "      1807  \n",
      "0      NaN  \n",
      "1    28.14  \n",
      "2      NaN  \n",
      "3    35.40  \n",
      "4    28.82  \n",
      "..     ...  \n",
      "255    NaN  \n",
      "256  32.60  \n",
      "257  33.70  \n",
      "258    NaN  \n",
      "259  26.67  \n",
      "\n",
      "[260 rows x 9 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "life = pd.read_csv(\"./data/life_expectancy_at_birth.csv\", usecols=(list(range(0, 10))), index_col=0).reindex() #usecols=list(range(10)),\n",
    "\n",
    "print(\"The starting dataframe: df --> life \\n\")\n",
    "print(life, \"\\n\")\n",
    "\n",
    "#print(\"Group by pclass (note that this is a DataFrameGroupBy object): ob --> by_class \\n\")\n",
    "#by_class = titanic.groupby(\"pclass\")\n",
    "#print(by_class, \"\\n\")\n",
    "\n",
    "#print(\"Calculate count by group pclass: sr --> count_by_pclass \\n\")\n",
    "#count_by_pclass = by_class[\"survived\"].count()\n",
    "#print(count_by_pclass, \"\\n\")\n",
    "\n",
    "#print(\"Group by embarked and pclass (note that this is also a DataFrameGroupBy object): ob --> by_embarked_pclass \\n\")\n",
    "#by_embarked_pclass = titanic.groupby([\"embarked\", \"pclass\"])\n",
    "#print(by_embarked_pclass, \"\\n\")\n",
    "\n",
    "#print(\"Calculate count by multi-group embarked and pclass: sr --> count_by_embarked_pclass \\n\")\n",
    "#count_by_embarked_pclass = by_embarked_pclass[\"survived\"].count()\n",
    "#print(count_by_embarked_pclass, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Bringing It All Together**</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
