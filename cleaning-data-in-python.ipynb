{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:maroon\">**Cleaning Data in Python**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Functions and Methods**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pandas functions\n",
    "> pandas.read_csv(file, header, names, index_col, parse_dates)  \n",
    "> pandas.melt(df, id_vars, value_vars, var_names, value_names)  \n",
    "> pandas.pivot_table(df, index columns, values, aggfunc)  \n",
    "> pandas.concat(list, ignore_index) **--> list is a list of dataframes**  \n",
    "> pandas.merge(left, right, left_on, right_on)  \n",
    "> pandas.to_numeric(df.column_name, errors)  **--> errors=\"coerce\" coerces type conversion errors to NaN**\n",
    "\n",
    "##### glob functions\n",
    "> glob.glob(pattern)  \n",
    "\n",
    "##### re functions\n",
    "> re.compile(regex)  **--> instantiates an re.Pattern**  \n",
    "> re.findall(regex, string) **--> useful when looking for numbers in text; in that case regex would be something like \"\\d+\"**  \n",
    "> re.match(regex, string)  \n",
    "\n",
    "##### re methods\n",
    "> re_Pattern_object.match(string)  **--> if string matches the regex then an re.Match object is returned else None is return**  \n",
    "\n",
    "##### dataframe methods\n",
    "> df.head(n)  \n",
    "> df.tail(n)  \n",
    "> df.info()  \n",
    "> df.describe()  \n",
    "> df[\"column_name\"].value_counts(dropna) **--> equivalent to -->** df.column_name.value_counts(dropna)  \n",
    "\n",
    "##### dataframe attributes\n",
    "> df.shape  \n",
    "> df.columns  \n",
    "> df.dtypes  \n",
    "> df.index\n",
    "\n",
    "##### dataframe manipulation\n",
    "> df[\"column_to_be_added\"] = df[\"pre-existing_column\"].str[0] **--> str is an example**  \n",
    "> df[\"split_column_name\"] = df[\"column_to_be_split\"].str.split(\"\\_\") **--> split_column_name is a list; \"\\_\" is an example**  \n",
    "> df[\"column_to_be_added] = df[\"split_column_name\"].str.get(n) **-> n is the index value for the list in \"split_column_name\"**  \n",
    "> df[\"column_name\"] = df[\"column_name\"].astype(\"category\")\n",
    "\n",
    "##### list manipulation\n",
    "> list.append(object) **--> appends an object to the tail end of a list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Exploring Your Data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Data Problems\n",
    "- Inconsistent column names\n",
    "- Missing data\n",
    "- Outliers\n",
    "- Duplicate rows\n",
    "- Untidy data structure\n",
    "- Need to process columns\n",
    "- Column types can signal unexpected data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"./data/stock_data.csv\"\n",
    "column_names = [\"ticker\", \"date\", \"close\", \"cap\", \"volume\"]\n",
    "df = pd.read_csv(file, header=1, names=column_names, index_col=\"date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df.head(), \"\\n\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(df.shape, \"\\n\")\n",
    "print(df.columns, \"\\n\")\n",
    "print(df.info(), \"\\n\")\n",
    "print(df.describe(), \"\\n\")\n",
    "print(df.ticker.value_counts(dropna=False), \"\\n\")\n",
    "print(df[\"ticker\"].value_counts(dropna=False), \"\\n\")     # same as previous lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Tidying Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tidy Data\n",
    "- \"Tidy Data\" - Hadley Wickham, Ph.D.\n",
    "- Formalize the way we describe the shape of data\n",
    "- Gives us a goal when formatting our data\n",
    "- Standard way to organize data values within a dataset\n",
    "- Certain data formats are better for reporting whereas other data formats are better for analysis\n",
    "- Easier to fix common data problems\n",
    "- Columns containing values, instead of variables\n",
    "\n",
    "##### Tidy Data Tenets\n",
    "- Columns represent separate variables\n",
    "- Rows represent individual observations\n",
    "- Observation units form tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tidy and Untidy Data\n",
    "import pandas as pd\n",
    "file = \"./data/airquality.csv\"\n",
    "airquality = pd.read_csv(file)\n",
    "print(airquality.head(), \"\\n\")                 # airquality is a tidy dataframe\n",
    "\n",
    "# convert the tidy dataframe airquality into an untidy dataframe using pd.melt()\n",
    "airquality_melt = pd.melt(airquality, id_vars=[\"Month\", \"Day\"], value_vars=[\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\"])\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "airquality_melt2 = pd.melt(airquality, id_vars=[\"Month\", \"Day\"], var_name=\"measurement\", value_name=\"reading\")\n",
    "print(airquality_melt2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivoting Data\n",
    "- Inverse of melting data\n",
    "- Pivoting takes an analysis-friendly shape (untidy) and creates a reporting-friendly shape (tidy)\n",
    "- Used to take a dataset that violates tidy data principle and converts to a tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "airquality_unmelted = pd.pivot_table(airquality_melt2, index=[\"Month\", \"Day\"],\n",
    "                                     columns=\"measurement\", values=\"reading\", aggfunc=np.mean)\n",
    "print(airquality_unmelted.head())\n",
    "# notice what happens to the \"Month\" index --> this is a hierarchical index - also known as a multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(airquality_unmelted.index, \"\\n\")\n",
    "airquality_reborn = airquality_unmelted.reset_index()\n",
    "print(airquality_reborn.index, \"\\n\")\n",
    "print(airquality_reborn.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tb = pd.read_csv(\"./data/tb.csv\")\n",
    "print(tb.head(), \"\\n\")\n",
    "\n",
    "tb_melt = pd.melt(tb, id_vars=[\"country\", \"year\"])\n",
    "tb_melt[\"gender\"] = tb_melt.variable.str[0]            # create a new column that is a calculation based on another column\n",
    "tb_melt[\"age_group\"] = tb_melt.variable.str[1:]        # create a new column that is a calculation based on another column\n",
    "\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ebola = pd.read_csv(\"./data/ebola.csv\")\n",
    "print(ebola.head(), \"\\n\")\n",
    "\n",
    "ebola_melt = pd.melt(ebola, id_vars=[\"Date\", \"Day\"], var_name=\"type_country\", value_name=\"counts\")\n",
    "print(ebola_melt.head(), \"\\n\")\n",
    "\n",
    "ebola_melt[\"str_split\"] = ebola_melt[\"type_country\"].str.split(\"_\")      # returns a list\n",
    "print(ebola_melt.head(), \"\\n\")\n",
    "\n",
    "ebola_melt[\"type\"] = ebola_melt[\"str_split\"].str.get(0)\n",
    "\n",
    "ebola_melt[\"country\"] = ebola_melt[\"str_split\"].str.get(1)\n",
    "\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Combining Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = \"./data/stock_data_1.csv\"\n",
    "file2 = \"./data/stock_data_2.csv\"\n",
    "column_names = [\"ticker\", \"date\", \"close\", \"cap\", \"volume\"]\n",
    "df1 = pd.read_csv(file1, header=1, names=column_names, index_col=\"date\", parse_dates=True)\n",
    "df2 = pd.read_csv(file2, header=1, names=column_names, index_col=\"date\", parse_dates=True)\n",
    "\n",
    "print(df1.shape, \"\\n\")\n",
    "print(df2.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)      # pd.concat requires a list\n",
    "                                                   # axis=0 concatenates row-wise (default), axis=1 concatenates column-wise\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def combine_dataframes(pattern, names, index_col, na_values, header=1, parse_dates=True, ignore_index=True):\n",
    "    # creates a list of file pointers matching the pattern\n",
    "    files = glob.glob(pattern)                        \n",
    "    frames = []\n",
    "   \n",
    "    # iterate through each file pointer\n",
    "    for f in files:\n",
    "    \n",
    "        # loads file into dataframe variable: df\n",
    "        df = pd.read_csv(f, names=names, index_col=index_col, na_values=na_values)                           \n",
    "        \n",
    "        # adds dataframe from df into list: frames\n",
    "        frames.append(df)  \n",
    "    \n",
    "    # concatenate all dataframes in frames into a single dataframe\n",
    "    return pd.concat(frames, ignore_index=ignore_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge left and right dataframes using a 1-to-1 join\n",
    "> o2o = pd.merge(left=site, right=visited, left_on=\"name\", right_on=\"site\")\n",
    "\n",
    "This is actually a full outer join. Note what happens when one of the dataframes does NOT match on the join.\\\n",
    "This behavior is the same for 1-to-1, many-to-1, and many-to-many joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Cleaning Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(tips.dtypes, \"\\n\")                               # gets the datatype of each column in a dataframe\n",
    "tips = pd.read_csv(\"./data/tips.csv\")\n",
    "print(tips.info(), \"\\n\")\n",
    "tips[\"sex\"] = tips[\"sex\"].astype(\"category\")           # note that this is the same --> tips.sex.astype(\"category\") = tips.sex.astype(\"category\")\n",
    "tips[\"smoker\"] = tips[\"smoker\"].astype(\"category\")\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tips.total_bill = pd.to_numeric(tips.total_bill, errors=\"coerce\")\n",
    "tips[\"total_bill\"] = pd.to_numeric(tips[\"total_bill\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile(\"\\d{3}-\\d{3}-\\d{4}\")\n",
    "\n",
    "# See if the pattern matches\n",
    "result1 = prog.match(\"123-456-7890\")\n",
    "print(bool(result1))\n",
    "\n",
    "# See if the pattern matches\n",
    "result2 = prog.match(\"1123-456-7890\")\n",
    "print(bool(result2))\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall(\"\\d+\", \"the recipe calls for 10 strawberries and 1 banana\")\n",
    "print(matches)\n",
    "\n",
    "pattern1 = bool(re.match(pattern=re.compile(\"^\\d{3}-\\d{3}-\\d{4}$\"), string=\"123-456-8330\"))\n",
    "print(pattern1)\n",
    "\n",
    "pattern2 = bool(re.match(pattern=\"^\\$\\d*\\.\\d{2}$\", string=\"$123.35\"))\n",
    "print(pattern2)\n",
    "\n",
    "pattern3 = bool(re.match(pattern=\"^[A-Z]\\w*$\", string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile(\"\\d{3}-\\d{3}-\\d{4}\")\n",
    "\n",
    "# See if the pattern matches\n",
    "result1 = prog.match(\"123-456-7890\")\n",
    "print(bool(result1))\n",
    "\n",
    "# See if the pattern matches\n",
    "result2 = prog.match(\"1123-456-7890\")\n",
    "print(bool(result2))\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall(\"\\d+\", \"the recipe calls for 10 strawberries and 1 banana\")\n",
    "print(matches)\n",
    "\n",
    "pattern1 = bool(re.match(pattern=re.compile(\"^\\d{3}-\\d{3}-\\d{4}$\"), string=\"123-456-8330\"))\n",
    "print(pattern1)\n",
    "\n",
    "pattern2 = bool(re.match(pattern=\"^\\$\\d*\\.\\d{2}$\", string=\"$123.35\"))\n",
    "print(pattern2)\n",
    "\n",
    "pattern3 = bool(re.match(pattern=\"^[A-Z]\\w*$\", string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Case Study**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Miscellaneous**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
