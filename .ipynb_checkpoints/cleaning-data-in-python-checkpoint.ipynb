{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:maroon\">**Cleaning Data in Python**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Functions and Methods**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pandas functions\n",
    "> pandas.read_csv(file, header, names, index_col, parse_dates)  \n",
    "> pandas.melt(df, id_vars, value_vars, var_names, value_names)  \n",
    "> pandas.pivot_table(df, index columns, values, aggfunc)  \n",
    "> pandas.concat(list, ignore_index) **--> list is a list of dataframes**  \n",
    "> pandas.merge(left, right, left_on, right_on)  \n",
    "> pandas.to_numeric(df.column_name, errors)  **--> errors=\"coerce\" coerces type conversion errors to NaN**\n",
    "\n",
    "##### glob functions\n",
    "> glob.glob(pattern)  \n",
    "\n",
    "##### re functions\n",
    "> re.compile(regex)  **--> instantiates an re.Pattern**  \n",
    "> re.findall(regex, string) **--> useful when looking for numbers in text; in that case regex would be something like \"\\d+\"**  \n",
    "> re.match(regex, string)  \n",
    "\n",
    "##### re methods\n",
    "> re_Pattern_object.match(string)  **--> if string matches the regex then an re.Match object is returned else None is return**  \n",
    "\n",
    "##### dataframe methods\n",
    "> df.head(n)  \n",
    "> df.tail(n)  \n",
    "> df.info()  \n",
    "> df.describe()  \n",
    "> df[\"column_name\"].value_counts(dropna) **--> equivalent to -->** df.column_name.value_counts(dropna)  \n",
    "\n",
    "##### dataframe attributes\n",
    "> df.shape  \n",
    "> df.columns  \n",
    "> df.dtypes  \n",
    "> df.index\n",
    "\n",
    "##### dataframe manipulation\n",
    "> df[\"column_to_be_added\"] = df[\"pre-existing_column\"].str[0] **--> str is an example**  \n",
    "> df[\"split_column_name\"] = df[\"column_to_be_split\"].str.split(\"\\_\") **--> split_column_name is a list; \"\\_\" is an example**  \n",
    "> df[\"column_to_be_added] = df[\"split_column_name\"].str.get(n) **-> n is the index value for the list in \"split_column_name\"**  \n",
    "> df[\"column_name\"] = df[\"column_name\"].astype(\"category\")\n",
    "\n",
    "##### list manipulation\n",
    "> list.append(object) **--> appends an object to the tail end of a list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Exploring Your Data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### Common Data Problems\n",
    "- Inconsistent column names\n",
    "- Missing data\n",
    "- Outliers\n",
    "- Duplicate rows\n",
    "- Untidy data structure\n",
    "- Need to process columns\n",
    "- Column types can signal unexpected data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"./data/stock_data.csv\"\n",
    "column_names = [\"ticker\", \"date\", \"close\", \"cap\", \"volume\"]\n",
    "df = pd.read_csv(file, header=1, names=column_names, index_col=\"date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df.head(), \"\\n\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(df.shape, \"\\n\")\n",
    "print(df.columns, \"\\n\")\n",
    "print(df.info(), \"\\n\")\n",
    "print(df.describe(), \"\\n\")\n",
    "print(df.ticker.value_counts(dropna=False), \"\\n\")\n",
    "print(df[\"ticker\"].value_counts(dropna=False), \"\\n\")     # same as previous lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Tidying Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tidy Data\n",
    "- \"Tidy Data\" - Hadley Wickham, Ph.D.\n",
    "- Formalize the way we describe the shape of data\n",
    "- Gives us a goal when formatting our data\n",
    "- Standard way to organize data values within a dataset\n",
    "- Certain data formats are better for reporting whereas other data formats are better for analysis\n",
    "- Easier to fix common data problems\n",
    "- Columns containing values, instead of variables\n",
    "\n",
    "##### Tidy Data Tenets\n",
    "- Columns represent separate variables\n",
    "- Rows represent individual observations\n",
    "- Observation units form tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tidy and Untidy Data\n",
    "import pandas as pd\n",
    "file = \"./data/airquality.csv\"\n",
    "airquality = pd.read_csv(file)\n",
    "print(airquality.head(), \"\\n\")                 # airquality is a tidy dataframe\n",
    "\n",
    "# convert the tidy dataframe airquality into an untidy dataframe using pd.melt()\n",
    "airquality_melt = pd.melt(airquality, id_vars=[\"Month\", \"Day\"], value_vars=[\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\"])\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "airquality_melt2 = pd.melt(airquality, id_vars=[\"Month\", \"Day\"], var_name=\"measurement\", value_name=\"reading\")\n",
    "print(airquality_melt2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivoting Data\n",
    "- Inverse of melting data\n",
    "- Pivoting takes an analysis-friendly shape (untidy) and creates a reporting-friendly shape (tidy)\n",
    "- Used to take a dataset that violates tidy data principle and converts to a tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "airquality_unmelted = pd.pivot_table(airquality_melt2, index=[\"Month\", \"Day\"],\n",
    "                                     columns=\"measurement\", values=\"reading\", aggfunc=np.mean)\n",
    "print(airquality_unmelted.head())\n",
    "# notice what happens to the \"Month\" index --> this is a hierarchical index - also known as a multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(airquality_unmelted.index, \"\\n\")\n",
    "airquality_reborn = airquality_unmelted.reset_index()\n",
    "print(airquality_reborn.index, \"\\n\")\n",
    "print(airquality_reborn.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  m014  m1524  m2534  m3544  m4554  m5564   m65  mu  f014  \\\n",
      "0      AD  2000   0.0    0.0    1.0    0.0    0.0    0.0   0.0 NaN   NaN   \n",
      "1      AE  2000   2.0    4.0    4.0    6.0    5.0   12.0  10.0 NaN   3.0   \n",
      "2      AF  2000  52.0  228.0  183.0  149.0  129.0   94.0  80.0 NaN  93.0   \n",
      "3      AG  2000   0.0    0.0    0.0    0.0    0.0    0.0   1.0 NaN   1.0   \n",
      "4      AL  2000   2.0   19.0   21.0   14.0   24.0   19.0  16.0 NaN   3.0   \n",
      "\n",
      "   f1524  f2534  f3544  f4554  f5564   f65  fu  \n",
      "0    NaN    NaN    NaN    NaN    NaN   NaN NaN  \n",
      "1   16.0    1.0    3.0    0.0    0.0   4.0 NaN  \n",
      "2  414.0  565.0  339.0  205.0   99.0  36.0 NaN  \n",
      "3    1.0    1.0    0.0    0.0    0.0   0.0 NaN  \n",
      "4   11.0   10.0    8.0    8.0    5.0  11.0 NaN   \n",
      "\n",
      "  country  year variable  value gender age_group\n",
      "0      AD  2000     m014    0.0      m       014\n",
      "1      AE  2000     m014    2.0      m       014\n",
      "2      AF  2000     m014   52.0      m       014\n",
      "3      AG  2000     m014    0.0      m       014\n",
      "4      AL  2000     m014    2.0      m       014\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_csv(\"./data/tb.csv\")\n",
    "print(tb.head(), \"\\n\")\n",
    "\n",
    "tb_melt = pd.melt(tb, id_vars=[\"country\", \"year\"])\n",
    "tb_melt[\"gender\"] = tb_melt.variable.str[0]            # create a new column that is a calculation based on another column\n",
    "tb_melt[\"age_group\"] = tb_melt.variable.str[1:]        # create a new column that is a calculation based on another column\n",
    "\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ebola = pd.read_csv(\"./data/ebola.csv\")\n",
    "print(ebola.head(), \"\\n\")\n",
    "\n",
    "ebola_melt = pd.melt(ebola, id_vars=[\"Date\", \"Day\"], var_name=\"type_country\", value_name=\"counts\")\n",
    "print(ebola_melt.head(), \"\\n\")\n",
    "\n",
    "ebola_melt[\"str_split\"] = ebola_melt[\"type_country\"].str.split(\"_\")      # returns a list\n",
    "print(ebola_melt.head(), \"\\n\")\n",
    "\n",
    "ebola_melt[\"type\"] = ebola_melt[\"str_split\"].str.get(0)\n",
    "\n",
    "ebola_melt[\"country\"] = ebola_melt[\"str_split\"].str.get(1)\n",
    "\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Combining Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = \"./data/stock_data_1.csv\"\n",
    "file2 = \"./data/stock_data_2.csv\"\n",
    "column_names = [\"ticker\", \"date\", \"close\", \"cap\", \"volume\"]\n",
    "df1 = pd.read_csv(file1, header=1, names=column_names, index_col=\"date\", parse_dates=True)\n",
    "df2 = pd.read_csv(file2, header=1, names=column_names, index_col=\"date\", parse_dates=True)\n",
    "\n",
    "print(df1.shape, \"\\n\")\n",
    "print(df2.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)      # pd.concat requires a list\n",
    "                                                   # axis=0 concatenates row-wise (default), axis=1 concatenates column-wise\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def combine_dataframes(pattern, names, index_col, na_values, header=1, parse_dates=True, ignore_index=True):\n",
    "    # creates a list of file pointers matching the pattern\n",
    "    files = glob.glob(pattern)                        \n",
    "    frames = []\n",
    "   \n",
    "    # iterate through each file pointer\n",
    "    for f in files:\n",
    "    \n",
    "        # loads file into dataframe variable: df\n",
    "        df = pd.read_csv(f, names=names, index_col=index_col, na_values=na_values)                           \n",
    "        \n",
    "        # adds dataframe from df into list: frames\n",
    "        frames.append(df)  \n",
    "    \n",
    "    # concatenate all dataframes in frames into a single dataframe\n",
    "    return pd.concat(frames, ignore_index=ignore_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge left and right dataframes using a 1-to-1 join\n",
    "> o2o = pd.merge(left=site, right=visited, left_on=\"name\", right_on=\"site\")\n",
    "\n",
    "This is actually a full outer join. Note what happens when one of the dataframes does NOT match on the join.\\\n",
    "This behavior is the same for 1-to-1, many-to-1, and many-to-many joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Cleaning Data for Analysis**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bill     float64\n",
      "tip            float64\n",
      "sex           category\n",
      "smoker        category\n",
      "day             object\n",
      "time            object\n",
      "size             int64\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      "total_bill    244 non-null float64\n",
      "tip           244 non-null float64\n",
      "sex           244 non-null object\n",
      "smoker        244 non-null object\n",
      "day           244 non-null object\n",
      "time          244 non-null object\n",
      "size          244 non-null int64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 13.5+ KB\n",
      "None \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      "total_bill    244 non-null float64\n",
      "tip           244 non-null float64\n",
      "sex           244 non-null category\n",
      "smoker        244 non-null category\n",
      "day           244 non-null object\n",
      "time          244 non-null object\n",
      "size          244 non-null int64\n",
      "dtypes: category(2), float64(2), int64(1), object(2)\n",
      "memory usage: 10.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(tips.dtypes, \"\\n\")                               # gets the datatype of each column in a dataframe\n",
    "tips = pd.read_csv(\"./data/tips.csv\")\n",
    "print(tips.info(), \"\\n\")\n",
    "tips[\"sex\"] = tips[\"sex\"].astype(\"category\")           # note that this is the same --> tips.sex.astype(\"category\") = tips.sex.astype(\"category\")\n",
    "tips[\"smoker\"] = tips[\"smoker\"].astype(\"category\")\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tips.total_bill = pd.to_numeric(tips.total_bill, errors=\"coerce\")\n",
    "tips[\"total_bill\"] = pd.to_numeric(tips[\"total_bill\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "['10', '1']\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile(\"\\d{3}-\\d{3}-\\d{4}\")\n",
    "\n",
    "# See if the pattern matches\n",
    "result1 = prog.match(\"123-456-7890\")\n",
    "print(bool(result1))\n",
    "\n",
    "# See if the pattern matches\n",
    "result2 = prog.match(\"1123-456-7890\")\n",
    "print(bool(result2))\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall(\"\\d+\", \"the recipe calls for 10 strawberries and 1 banana\")\n",
    "print(matches)\n",
    "\n",
    "pattern1 = bool(re.match(pattern=re.compile(\"\\d{3}-\\d{3}-\\d{4}\"), string=\"123-456-2780\"))\n",
    "print(pattern1)\n",
    "\n",
    "pattern2 = bool(re.match(pattern=\"\\$\\d*\\.\\d{2}\", string=\"$123.45\"))\n",
    "print(pattern2)\n",
    "\n",
    "pattern3 = bool(re.match(pattern=\"[A-Z]\\w*\", string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Case Study**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">**Miscellaneous**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
